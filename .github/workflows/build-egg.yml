name: Build Bluefin Egg

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

env:
  IMAGE_NAME: egg
  IMAGE_REGISTRY: ghcr.io/${{ github.repository_owner }}
  BST2_IMAGE: registry.gitlab.com/freedesktop-sdk/infrastructure/freedesktop-sdk-docker-images/bst2:f89b4aef847ef040b345acceda15a850219eb8f1
  R2_BUCKET: bst-cache

# On PRs: group by branch so new pushes cancel stale runs.
# On main: group by SHA so every push gets its own non-cancellable run.
# (GitHub cancels even pending/queued runs in the same group regardless of
#  cancel-in-progress when a newer run arrives, so main needs unique groups.)
concurrency:
  group: ${{ github.workflow }}-${{ github.event_name == 'pull_request' && github.ref || github.sha }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

jobs:
  build:
    runs-on: Testing
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Pull BuildStream container image
        run: podman pull "$BST2_IMAGE"

      - name: Cache BuildStream sources
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        with:
          path: ~/.cache/buildstream/sources
          key: bst-sources-${{ hashFiles('elements/**/*.bst', 'project.conf') }}
          restore-keys: |
            bst-sources-

      - name: Prepare BuildStream cache directory
        run: |
          mkdir -p "$HOME/.cache/buildstream/sources"
          mkdir -p "$HOME/.cache/buildstream/cas"
          mkdir -p "$HOME/.cache/buildstream/artifacts"
          mkdir -p "$HOME/.cache/buildstream/source_protos"

      # ── Restore cache from R2 ─────────────────────────────────────────
      # CAS objects are stored as a single zstd-compressed tar in R2.
      # Artifact refs and source protos are small metadata dirs (~12MB).

      - name: Install rclone
        run: |
          curl -fsSL https://rclone.org/install.sh | sudo bash
          rclone version

      - name: Install just
        run: |
          # just is in ubuntu 24.04 universe; skopeo/podman are pre-installed
          sudo apt-get install -y just
          just --version

      - name: Restore BuildStream cache from R2
        env:
          R2_ACCESS_KEY: ${{ secrets.R2_ACCESS_KEY }}
          R2_SECRET_KEY: ${{ secrets.R2_SECRET_KEY }}
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
        run: |
          if [ -z "${R2_ACCESS_KEY}" ]; then
            echo "R2 secrets not configured, skipping cache restore"
            exit 0
          fi

          # Configure rclone for Cloudflare R2 (S3-compatible)
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<EOF
          [r2]
          type = s3
          provider = Cloudflare
          access_key_id = $R2_ACCESS_KEY
          secret_access_key = $R2_SECRET_KEY
          endpoint = $R2_ENDPOINT
          no_check_bucket = true
          EOF
          # Strip leading whitespace from config (heredoc is indented in YAML)
          sed -i 's/^[[:space:]]*//' ~/.config/rclone/rclone.conf

          BST_CACHE="$HOME/.cache/buildstream"
          RESTORE_START=$SECONDS
          
          # Record cache state before restore
          echo "=== Cache state before restore ==="
          CAS_BEFORE=$(du -sb "${BST_CACHE}/cas" 2>/dev/null | cut -f1 || echo 0)
          ARTIFACTS_BEFORE=$(du -sb "${BST_CACHE}/artifacts" 2>/dev/null | cut -f1 || echo 0)
          echo "CAS: $((CAS_BEFORE / 1048576)) MB"
          echo "Artifacts: $((ARTIFACTS_BEFORE / 1048576)) MB"

          echo ""
          echo "=== Restoring CAS from R2 (tar+zstd) ==="
          CAS_RESTORED=false
          if rclone lsf "r2:${R2_BUCKET}/cas.tar.zst" 2>&1; then
            # Get file size for reporting
            CAS_REMOTE_SIZE=$(rclone size --json "r2:${R2_BUCKET}/cas.tar.zst" 2>/dev/null | jq -r '.bytes // 0')
            echo "Remote cas.tar.zst size: $((CAS_REMOTE_SIZE / 1048576)) MB"
            
            # Download and validate tar+zstd stream
            # Use temporary file to validate before extracting
            TEMP_CAS="/tmp/cas.tar.zst"
            echo "Downloading cas.tar.zst..."
            if rclone copy "r2:${R2_BUCKET}/cas.tar.zst" /tmp/ --s3-no-head-object --progress 2>&1; then
              echo "Validating archive integrity..."
              if zstd -t "$TEMP_CAS" 2>&1; then
                echo "Archive validation passed, extracting..."
                if zstd -d "$TEMP_CAS" | tar xf - -C "${BST_CACHE}/"; then
                  CAS_RESTORED=true
                  echo "✓ CAS restore completed in $(( SECONDS - RESTORE_START ))s"
                else
                  echo "::warning::CAS tar extraction failed (corrupted archive)"
                fi
              else
                echo "::warning::CAS archive validation failed (corrupted zstd)"
                echo "Deleting corrupted cache file from R2..."
                rclone delete "r2:${R2_BUCKET}/cas.tar.zst" 2>&1 || true
              fi
              rm -f "$TEMP_CAS"
            else
              echo "::warning::CAS download failed"
            fi
          else
            echo "No cas.tar.zst found in R2, skipping CAS restore"
          fi

          echo ""
          echo "=== Restoring artifact refs from R2 ==="
          rclone copy "r2:${R2_BUCKET}/artifacts/" "${BST_CACHE}/artifacts/" \
            --size-only --transfers=16 --fast-list \
            --s3-no-head-object --s3-no-system-metadata \
            -v || echo "::warning::Artifact refs restore failed (non-fatal)"

          echo ""
          echo "=== Restoring source protos from R2 ==="
          rclone copy "r2:${R2_BUCKET}/source_protos/" "${BST_CACHE}/source_protos/" \
            --size-only --transfers=16 --fast-list \
            --s3-no-head-object --s3-no-system-metadata \
            -v || echo "::warning::Source protos restore failed (non-fatal)"

          echo ""
          echo "╔══════════════════════════════════════════════════════════════╗"
          echo "║              CACHE RESTORE REPORT                            ║"
          echo "╚══════════════════════════════════════════════════════════════╝"
          
          # Calculate cache sizes after restore
          CAS_AFTER=$(du -sb "${BST_CACHE}/cas" 2>/dev/null | cut -f1 || echo 0)
          ARTIFACTS_AFTER=$(du -sb "${BST_CACHE}/artifacts" 2>/dev/null | cut -f1 || echo 0)
          PROTOS_AFTER=$(du -sb "${BST_CACHE}/source_protos" 2>/dev/null | cut -f1 || echo 0)
          SOURCES_AFTER=$(du -sb "${BST_CACHE}/sources" 2>/dev/null | cut -f1 || echo 0)
          
          CAS_DELTA=$((CAS_AFTER - CAS_BEFORE))
          ARTIFACTS_DELTA=$((ARTIFACTS_AFTER - ARTIFACTS_BEFORE))
          
          echo "Component Sizes:"
          echo "  CAS (objects):        $((CAS_AFTER / 1048576)) MB (+$((CAS_DELTA / 1048576)) MB)"
          echo "  Artifacts (refs):     $((ARTIFACTS_AFTER / 1048576)) MB (+$((ARTIFACTS_DELTA / 1048576)) MB)"
          echo "  Source protos:        $((PROTOS_AFTER / 1048576)) MB"
          echo "  Sources (from GHA):   $((SOURCES_AFTER / 1048576)) MB"
          echo ""
          
          if [ "$CAS_RESTORED" = "true" ]; then
            echo "Cache Status: ✓ RESTORED (CAS validated and extracted)"
          elif [ "$CAS_AFTER" -lt 10485760 ]; then
            echo "Cache Status: ✗ EMPTY (cold build, will fetch from upstream)"
          else
            echo "Cache Status: ⚠ PARTIAL (CAS restore failed, using previous cache)"
          fi
          
          echo ""
          echo "Total cache: $(du -sh "${BST_CACHE}" 2>/dev/null | cut -f1)"
          echo "Time: $(( SECONDS - RESTORE_START ))s"
          echo "═══════════════════════════════════════════════════════════════"
          
          df -h /

      # ── Generate CI-specific BuildStream config ───────────────────────
      # Tuned per gnome-build-meta CI patterns:
      # - on-error: continue  -> find ALL failures, don't stop at first
      # - fetchers: 12        -> parallel downloads from upstream caches
      # - builders: 1         -> GHA has 4 vCPUs; single builder avoids OOM
      # - retry-failed: True  -> auto-retry flaky builds
      # - error-lines: 80     -> generous error context in logs
      # - cache-buildtrees: never -> save disk (we only need final artifacts)
      #
      # No remote artifact server is configured. BuildStream uses only:
      # 1. Local disk cache (restored from R2 above)
      # 2. Upstream GNOME caches defined in project.conf (read-only)
      # After the build, rclone syncs everything back to R2.

      - name: Generate BuildStream CI config
        run: |
          mkdir -p logs
          cat > buildstream-ci.conf <<'BSTCONF'
          scheduler:
            on-error: continue
            fetchers: 12
            builders: 1
            network-retries: 3

          logging:
            message-format: '[%{wallclock}][%{elapsed}][%{key}][%{element}] %{action} %{message}'
            error-lines: 80

          build:
            max-jobs: 0
            retry-failed: True

          cache:
            cache-buildtrees: never
          BSTCONF

          echo "=== BuildStream CI config ==="
          cat buildstream-ci.conf

      # ── Background R2 sync ──────────────────────────────────────────
      # Upload a tar+zstd snapshot of the CAS to R2 every 5 minutes.
      # This ensures partial progress is saved even if the build times
      # out or the runner is lost. Also syncs artifact/source metadata.
      # Uses set +e (no errexit) so the loop survives upload failures.

      - name: Start background R2 sync
        env:
          R2_ACCESS_KEY: ${{ secrets.R2_ACCESS_KEY }}
          R2_SECRET_KEY: ${{ secrets.R2_SECRET_KEY }}
        run: |
          if [ -z "${R2_ACCESS_KEY}" ]; then
            echo "R2 secrets not configured, skipping background sync"
            exit 0
          fi

          BST_CACHE="$HOME/.cache/buildstream"

          cat > /tmp/r2-sync-loop.sh <<'SYNCSCRIPT'
          #!/usr/bin/env bash
          # NO set -e or pipefail — the loop must survive failures
          BST_CACHE="$1"
          R2_BUCKET="$2"
          INTERVAL=300  # every 5 minutes

          echo "[r2-sync] Background sync started (every ${INTERVAL}s)"
          echo "[r2-sync] BST_CACHE=${BST_CACHE} R2_BUCKET=${R2_BUCKET}"
          echo "[r2-sync] Using atomic upload strategy (temp file + rename)"

          while true; do
            sleep "$INTERVAL"

            CAS_SIZE=$(du -sb "${BST_CACHE}/cas" 2>/dev/null | cut -f1 || echo 0)
            CAS_MB=$(( CAS_SIZE / 1048576 ))
            echo "[r2-sync] $(date -Iseconds) CAS size: ${CAS_MB} MB"

            if [ "$CAS_MB" -lt 100 ]; then
              echo "[r2-sync] CAS too small (${CAS_MB} MB), skipping upload"
              continue
            fi

            echo "[r2-sync] $(date -Iseconds) Starting atomic tar+zstd upload..."
            # Upload to temporary name first, then rename atomically
            # This prevents corrupted partial uploads from being used
            TEMP_NAME="cas.tar.zst.uploading.$$"
            if tar cf - -C "${BST_CACHE}/" cas/ 2>/dev/null \
              | zstd -T0 -3 2>/dev/null \
              | rclone rcat \
                  --streaming-upload-cutoff 0 \
                  --s3-chunk-size=64M \
                  --s3-upload-concurrency=4 \
                  "r2:${R2_BUCKET}/${TEMP_NAME}" 2>&1; then
              echo "[r2-sync] Upload complete, atomically renaming..."
              if rclone moveto "r2:${R2_BUCKET}/${TEMP_NAME}" "r2:${R2_BUCKET}/cas.tar.zst" 2>&1; then
                echo "[r2-sync] $(date -Iseconds) ✓ CAS snapshot uploaded (${CAS_MB} MB uncompressed)"
              else
                echo "[r2-sync] $(date -Iseconds) Rename failed, cleaning up temp file"
                rclone delete "r2:${R2_BUCKET}/${TEMP_NAME}" 2>&1 || true
              fi
            else
              echo "[r2-sync] $(date -Iseconds) Upload failed (exit=$?), cleaning up"
              rclone delete "r2:${R2_BUCKET}/${TEMP_NAME}" 2>&1 || true
            fi

            # Sync metadata (small, fast)
            rclone copy "${BST_CACHE}/artifacts/" "r2:${R2_BUCKET}/artifacts/" \
              --no-traverse --size-only --transfers=8 --s3-no-head -q 2>&1 || true
            rclone copy "${BST_CACHE}/source_protos/" "r2:${R2_BUCKET}/source_protos/" \
              --no-traverse --size-only --transfers=8 --s3-no-head -q 2>&1 || true

            echo "[r2-sync] $(date -Iseconds) Sync cycle complete"
          done
          SYNCSCRIPT
          chmod +x /tmp/r2-sync-loop.sh

          nohup /tmp/r2-sync-loop.sh "$BST_CACHE" "$R2_BUCKET" \
            > /tmp/r2-sync-loop.log 2>&1 &
          echo $! > /tmp/r2-sync-loop.pid
          echo "Background R2 sync started (PID $(cat /tmp/r2-sync-loop.pid))"

      # ── BuildStream build ─────────────────────────────────────────────
      # Uses the Justfile's `bst` wrapper to run BuildStream inside the
      # bst2 container. CI-specific flags (--no-interactive, --config)
      # are injected via BST_FLAGS env var.
      # Additional --log-file flag is passed directly (build-specific).

      - name: Build OCI image with BuildStream
        env:
          BST_FLAGS: --no-interactive --config /src/buildstream-ci.conf
        run: |
          just bst --log-file /src/logs/build.log build oci/bluefin.bst
        timeout-minutes: 120

      - name: Disk and cache usage after build
        if: always()
        run: |
          echo "=== Disk usage ==="
          df -h /
          echo "=== BuildStream cache breakdown ==="
          du -sh ~/.cache/buildstream/{cas,artifacts,source_protos,sources,logs,build,tmp} 2>/dev/null || true
          echo "=== Total cache ==="
          du -sh ~/.cache/buildstream/ 2>/dev/null || true

      # ── Final R2 sync ───────────────────────────────────────────────
      # Stop the background sync loop, then upload the definitive cache.
      # CAS is uploaded as a single tar+zstd archive (replaces individual files).
      # Metadata dirs (artifacts, source_protos) are synced per-file (small, ~12MB).
      # Always runs (even on build failure) so partial progress is saved.

      - name: Final sync to R2
        if: always()
        continue-on-error: true
        env:
          R2_ACCESS_KEY: ${{ secrets.R2_ACCESS_KEY }}
          R2_SECRET_KEY: ${{ secrets.R2_SECRET_KEY }}
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
        run: |
          # Stop background sync loop
          if [ -f /tmp/r2-sync-loop.pid ]; then
            PID=$(cat /tmp/r2-sync-loop.pid)
            if kill -0 "$PID" 2>/dev/null; then
              echo "Stopping background R2 sync (PID $PID)"
              kill "$PID" || true
              wait "$PID" 2>/dev/null || true
            fi
            rm -f /tmp/r2-sync-loop.pid
          fi

          echo "=== Background sync log (last 30 lines) ==="
          tail -30 /tmp/r2-sync-loop.log 2>/dev/null || echo "(no log)"

          if [ -z "${R2_ACCESS_KEY}" ]; then
            echo "R2 secrets not configured, skipping final sync"
            exit 0
          fi

          BST_CACHE="$HOME/.cache/buildstream"
          SYNC_START=$SECONDS

          echo ""
          echo "=== Local cache sizes before upload ==="
          CAS_SIZE=$(du -sb "${BST_CACHE}/cas" 2>/dev/null | cut -f1 || echo 0)
          ARTIFACTS_SIZE=$(du -sb "${BST_CACHE}/artifacts" 2>/dev/null | cut -f1 || echo 0)
          PROTOS_SIZE=$(du -sb "${BST_CACHE}/source_protos" 2>/dev/null | cut -f1 || echo 0)
          
          echo "CAS:           $((CAS_SIZE / 1048576)) MB"
          echo "Artifacts:     $((ARTIFACTS_SIZE / 1048576)) MB"
          echo "Source protos: $((PROTOS_SIZE / 1048576)) MB"

          echo ""
          echo "=== Final CAS sync to R2 (atomic tar+zstd upload) ==="
          # Upload entire CAS dir as a single zstd-compressed tar archive.
          # Uses atomic upload strategy: upload to temp name, then rename.
          # This prevents corrupted partial uploads from being restored.
          # tar -> zstd (level 9, all threads) -> rclone rcat (streaming multipart upload)
          # Level 9: ~3.2x ratio vs ~2.8x at level 3, saving ~3-4 GB on R2 per archive.
          # Costs ~2-3 min extra compression time (4 cores) but decompression is the same
          # speed at any level (~1.5 GB/s), so every restore benefits from the smaller file.
          # --streaming-upload-cutoff 0: force immediate multipart (don't buffer in RAM)
          # This replaces ~20,000 individual PUT requests with a single multipart upload.
          CAS_UPLOAD_START=$SECONDS
          TEMP_NAME="cas.tar.zst.uploading.$$"
          CAS_UPLOADED=false
          
          if tar cf - -C "${BST_CACHE}/" cas/ \
            | zstd -T0 -9 \
            | rclone rcat \
                --streaming-upload-cutoff 0 \
                --s3-chunk-size=64M \
                --s3-upload-concurrency=4 \
                "r2:${R2_BUCKET}/${TEMP_NAME}"; then
            echo "Upload complete, atomically renaming..."
            if rclone moveto "r2:${R2_BUCKET}/${TEMP_NAME}" "r2:${R2_BUCKET}/cas.tar.zst"; then
              CAS_UPLOADED=true
              echo "✓ CAS upload completed in $(( SECONDS - CAS_UPLOAD_START ))s"
            else
              echo "::warning::Atomic rename failed, cleaning up temp file"
              rclone delete "r2:${R2_BUCKET}/${TEMP_NAME}" || true
            fi
          else
            echo "::warning::CAS tar+zstd upload to R2 failed"
            rclone delete "r2:${R2_BUCKET}/${TEMP_NAME}" 2>&1 || true
          fi

          echo ""
          echo "=== Final artifact refs sync to R2 ==="
          ARTIFACTS_UPLOAD_START=$SECONDS
          ARTIFACTS_UPLOADED=false
          if rclone sync "${BST_CACHE}/artifacts/" "r2:${R2_BUCKET}/artifacts/" \
            --size-only \
            --transfers=16 \
            --checkers=8 \
            --fast-list \
            --s3-no-head \
            --s3-no-system-metadata \
            -v; then
            ARTIFACTS_UPLOADED=true
            echo "✓ Artifact refs synced in $(( SECONDS - ARTIFACTS_UPLOAD_START ))s"
          else
            echo "::warning::Artifact refs sync to R2 failed"
          fi

          echo ""
          echo "=== Final source protos sync to R2 ==="
          PROTOS_UPLOAD_START=$SECONDS
          PROTOS_UPLOADED=false
          if rclone sync "${BST_CACHE}/source_protos/" "r2:${R2_BUCKET}/source_protos/" \
            --size-only \
            --transfers=16 \
            --checkers=8 \
            --fast-list \
            --s3-no-head \
            --s3-no-system-metadata \
            -v; then
            PROTOS_UPLOADED=true
            echo "✓ Source protos synced in $(( SECONDS - PROTOS_UPLOAD_START ))s"
          else
            echo "::warning::Source protos sync to R2 failed"
          fi

          # Get remote sizes for reporting
          echo ""
          echo "Checking remote cache sizes..."
          CAS_REMOTE_SIZE=$(rclone size --json "r2:${R2_BUCKET}/cas.tar.zst" 2>/dev/null | jq -r '.bytes // 0')
          ARTIFACTS_REMOTE_COUNT=$(rclone size --json "r2:${R2_BUCKET}/artifacts/" 2>/dev/null | jq -r '.count // 0')
          PROTOS_REMOTE_COUNT=$(rclone size --json "r2:${R2_BUCKET}/source_protos/" 2>/dev/null | jq -r '.count // 0')

          echo ""
          echo "╔══════════════════════════════════════════════════════════════╗"
          echo "║              CACHE UPLOAD REPORT                             ║"
          echo "╚══════════════════════════════════════════════════════════════╝"
          echo ""
          echo "Component Upload Status:"
          if [ "$CAS_UPLOADED" = "true" ]; then
            echo "  CAS (objects):        ✓ UPLOADED ($((CAS_SIZE / 1048576)) MB → $((CAS_REMOTE_SIZE / 1048576)) MB compressed)"
            # Calculate compression ratio
            if [ "$CAS_SIZE" -gt 0 ]; then
              RATIO=$(( (CAS_SIZE * 100) / CAS_REMOTE_SIZE ))
              echo "                        Compression ratio: ${RATIO}% (zstd level 9)"
            fi
          else
            echo "  CAS (objects):        ✗ FAILED"
          fi
          
          if [ "$ARTIFACTS_UPLOADED" = "true" ]; then
            echo "  Artifacts (refs):     ✓ UPLOADED ($((ARTIFACTS_SIZE / 1048576)) MB, ${ARTIFACTS_REMOTE_COUNT} files)"
          else
            echo "  Artifacts (refs):     ✗ FAILED"
          fi
          
          if [ "$PROTOS_UPLOADED" = "true" ]; then
            echo "  Source protos:        ✓ UPLOADED ($((PROTOS_SIZE / 1048576)) MB, ${PROTOS_REMOTE_COUNT} files)"
          else
            echo "  Source protos:        ✗ FAILED"
          fi
          
          echo ""
          echo "Cache Health:"
          if [ "$CAS_UPLOADED" = "true" ] && [ "$ARTIFACTS_UPLOADED" = "true" ]; then
            echo "  Status: ✓ COMPLETE (next build will have warm cache)"
          elif [ "$CAS_UPLOADED" = "true" ]; then
            echo "  Status: ⚠ PARTIAL (CAS ok, but metadata sync failed)"
          elif [ "$ARTIFACTS_UPLOADED" = "true" ]; then
            echo "  Status: ⚠ PARTIAL (metadata ok, but CAS failed - cold build next time)"
          else
            echo "  Status: ✗ FAILED (next build will be cold)"
          fi
          
          echo ""
          echo "Total time: $(( SECONDS - SYNC_START ))s"
          echo "═══════════════════════════════════════════════════════════════"
          echo ""
          echo "Cache Efficiency Note:"
          echo "• Warm cache (CAS restored): ~30-40 min build time"
          echo "• Cold cache (no CAS):       ~120+ min build time"
          echo "• Cache validated on restore to prevent corruption"
          echo "• Atomic uploads prevent partial file corruption"

      # ── Export OCI image ──────────────────────────────────────────────
      # Uses the Justfile's `export` recipe: checks out the OCI image
      # layout from BuildStream, loads it into podman via skopeo, and
      # applies the /usr/etc bootc fixup. Same logic as local builds.
      # Creates image with local name (egg:latest) to avoid registry
      # lookups during validation.

      - name: Export OCI image from BuildStream
        id: export
        env:
          BST_FLAGS: --no-interactive --config /src/buildstream-ci.conf
          BUILD_IMAGE_NAME: ${{ env.IMAGE_NAME }}
        run: |
          just export
          echo "image_ref=${{ env.IMAGE_NAME }}:latest" >> "$GITHUB_OUTPUT"

      - name: Verify image loaded
        run: podman images

      # ── Validation ────────────────────────────────────────────────────

      - name: Validate with bootc container lint
        run: |
          podman run --rm --privileged \
            -v /var/lib/containers:/var/lib/containers \
            "${{ steps.export.outputs.image_ref }}" \
            bootc container lint

      # ── Upload build logs ─────────────────────────────────────────────
      # Always upload, even on failure, so build failures can be diagnosed.

      - name: Upload build logs
        if: always()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
        with:
          name: buildstream-logs
          path: logs/
          retention-days: 7
          if-no-files-found: ignore

      # ── Publish to GHCR (main branch only) ───────────────────────────

      - name: Login to GHCR
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          echo "${{ secrets.GITHUB_TOKEN }}" | \
            podman login ghcr.io --username ${{ github.actor }} --password-stdin

      - name: Tag image for GHCR
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          podman tag "${{ steps.export.outputs.image_ref }}" \
            "${{ env.IMAGE_REGISTRY }}/${{ env.IMAGE_NAME }}:latest"
          podman tag "${{ steps.export.outputs.image_ref }}" \
            "${{ env.IMAGE_REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}"

      - name: Push to GHCR
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          podman push --retry 3 "${{ env.IMAGE_REGISTRY }}/${{ env.IMAGE_NAME }}:latest"
          podman push --retry 3 "${{ env.IMAGE_REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}"
