name: Build Bluefin Egg

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

env:
  IMAGE_NAME: egg
  IMAGE_REGISTRY: ghcr.io/${{ github.repository_owner }}
  BST2_IMAGE: registry.gitlab.com/freedesktop-sdk/infrastructure/freedesktop-sdk-docker-images/bst2:f89b4aef847ef040b345acceda15a850219eb8f1
  R2_BUCKET: bst-cache

# On PRs: group by branch so new pushes cancel stale runs.
# On main: group by SHA so every push gets its own non-cancellable run.
# (GitHub cancels even pending/queued runs in the same group regardless of
#  cancel-in-progress when a newer run arrives, so main needs unique groups.)
concurrency:
  group: ${{ github.workflow }}-${{ github.event_name == 'pull_request' && github.ref || github.sha }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

jobs:
  build:
    runs-on: Testing
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Pull BuildStream container image
        run: podman pull "$BST2_IMAGE"

      - name: Cache BuildStream sources
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        with:
          path: ~/.cache/buildstream/sources
          key: bst-sources-${{ hashFiles('elements/**/*.bst', 'project.conf') }}
          restore-keys: |
            bst-sources-

      - name: Prepare BuildStream cache directory
        run: |
          mkdir -p "$HOME/.cache/buildstream/sources"
          mkdir -p "$HOME/.cache/buildstream/cas"
          mkdir -p "$HOME/.cache/buildstream/artifacts"
          mkdir -p "$HOME/.cache/buildstream/source_protos"

      # ── Restore cache from R2 ─────────────────────────────────────────
      # CAS objects are stored as a single zstd-compressed tar in R2.
      # Artifact refs and source protos are small metadata dirs (~12MB).

      - name: Install rclone
        run: |
          curl -fsSL https://rclone.org/install.sh | sudo bash
          rclone version

      - name: Install just
        run: |
          # just is in ubuntu 24.04 universe; skopeo/podman are pre-installed
          sudo apt-get install -y just
          just --version

      - name: Restore BuildStream cache from R2
        env:
          R2_ACCESS_KEY: ${{ secrets.R2_ACCESS_KEY }}
          R2_SECRET_KEY: ${{ secrets.R2_SECRET_KEY }}
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
        run: |
          if [ -z "${R2_ACCESS_KEY}" ]; then
            echo "R2 secrets not configured, skipping cache restore"
            exit 0
          fi

          # Configure rclone for Cloudflare R2 (S3-compatible)
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<EOF
          [r2]
          type = s3
          provider = Cloudflare
          access_key_id = $R2_ACCESS_KEY
          secret_access_key = $R2_SECRET_KEY
          endpoint = $R2_ENDPOINT
          no_check_bucket = true
          EOF
          # Strip leading whitespace from config (heredoc is indented in YAML)
          sed -i 's/^[[:space:]]*//' ~/.config/rclone/rclone.conf

          BST_CACHE="$HOME/.cache/buildstream"
          RESTORE_START=$SECONDS

          echo "=== Restoring CAS from R2 (tar+zstd) ==="
          if rclone lsf "r2:${R2_BUCKET}/cas.tar.zst" 2>&1; then
            if rclone cat "r2:${R2_BUCKET}/cas.tar.zst" \
              --s3-no-head-object \
              | zstd -d \
              | tar xf - -C "${BST_CACHE}/"; then
              echo "CAS restore completed in $(( SECONDS - RESTORE_START ))s"
            else
              echo "::warning::CAS tar+zstd restore failed (non-fatal)"
            fi
          else
            echo "No cas.tar.zst found in R2, skipping CAS restore"
          fi

          echo "=== Restoring artifact refs from R2 ==="
          rclone copy "r2:${R2_BUCKET}/artifacts/" "${BST_CACHE}/artifacts/" \
            --size-only --transfers=16 --fast-list \
            --s3-no-head-object --s3-no-system-metadata \
            -v || echo "::warning::Artifact refs restore failed (non-fatal)"

          echo "=== Restoring source protos from R2 ==="
          rclone copy "r2:${R2_BUCKET}/source_protos/" "${BST_CACHE}/source_protos/" \
            --size-only --transfers=16 --fast-list \
            --s3-no-head-object --s3-no-system-metadata \
            -v || echo "::warning::Source protos restore failed (non-fatal)"

          echo "=== Cache restore summary ==="
          du -sh "${BST_CACHE}/cas" "${BST_CACHE}/artifacts" \
                 "${BST_CACHE}/source_protos" "${BST_CACHE}/sources" 2>/dev/null || true
          df -h /

      # ── Generate CI-specific BuildStream config ───────────────────────
      # Tuned per gnome-build-meta CI patterns:
      # - on-error: continue  -> find ALL failures, don't stop at first
      # - fetchers: 12        -> parallel downloads from upstream caches
      # - builders: 1         -> GHA has 4 vCPUs; single builder avoids OOM
      # - retry-failed: True  -> auto-retry flaky builds
      # - error-lines: 80     -> generous error context in logs
      # - cache-buildtrees: never -> save disk (we only need final artifacts)
      #
      # No remote artifact server is configured. BuildStream uses only:
      # 1. Local disk cache (restored from R2 above)
      # 2. Upstream GNOME caches defined in project.conf (read-only)
      # After the build, rclone syncs everything back to R2.

      - name: Generate BuildStream CI config
        run: |
          mkdir -p logs
          cat > buildstream-ci.conf <<'BSTCONF'
          scheduler:
            on-error: continue
            fetchers: 12
            builders: 1
            network-retries: 3

          logging:
            message-format: '[%{wallclock}][%{elapsed}][%{key}][%{element}] %{action} %{message}'
            error-lines: 80

          build:
            max-jobs: 0
            retry-failed: True

          cache:
            cache-buildtrees: never
          BSTCONF

          echo "=== BuildStream CI config ==="
          cat buildstream-ci.conf

      # ── Background R2 sync ──────────────────────────────────────────
      # Upload a tar+zstd snapshot of the CAS to R2 every 5 minutes.
      # This ensures partial progress is saved even if the build times
      # out or the runner is lost. Also syncs artifact/source metadata.
      # Uses set +e (no errexit) so the loop survives upload failures.

      - name: Start background R2 sync
        env:
          R2_ACCESS_KEY: ${{ secrets.R2_ACCESS_KEY }}
          R2_SECRET_KEY: ${{ secrets.R2_SECRET_KEY }}
        run: |
          if [ -z "${R2_ACCESS_KEY}" ]; then
            echo "R2 secrets not configured, skipping background sync"
            exit 0
          fi

          BST_CACHE="$HOME/.cache/buildstream"

          cat > /tmp/r2-sync-loop.sh <<'SYNCSCRIPT'
          #!/usr/bin/env bash
          # NO set -e or pipefail — the loop must survive failures
          BST_CACHE="$1"
          R2_BUCKET="$2"
          INTERVAL=300  # every 5 minutes

          echo "[r2-sync] Background sync started (every ${INTERVAL}s)"
          echo "[r2-sync] BST_CACHE=${BST_CACHE} R2_BUCKET=${R2_BUCKET}"

          while true; do
            sleep "$INTERVAL"

            CAS_SIZE=$(du -sb "${BST_CACHE}/cas" 2>/dev/null | cut -f1 || echo 0)
            CAS_MB=$(( CAS_SIZE / 1048576 ))
            echo "[r2-sync] $(date -Iseconds) CAS size: ${CAS_MB} MB"

            if [ "$CAS_MB" -lt 100 ]; then
              echo "[r2-sync] CAS too small (${CAS_MB} MB), skipping upload"
              continue
            fi

            echo "[r2-sync] $(date -Iseconds) Starting tar+zstd upload..."
            if tar cf - -C "${BST_CACHE}/" cas/ 2>/dev/null \
              | zstd -T0 -3 2>/dev/null \
              | rclone rcat \
                  --streaming-upload-cutoff 0 \
                  --s3-chunk-size=64M \
                  --s3-upload-concurrency=4 \
                  "r2:${R2_BUCKET}/cas.tar.zst" 2>&1; then
              echo "[r2-sync] $(date -Iseconds) CAS snapshot uploaded (${CAS_MB} MB uncompressed)"
            else
              echo "[r2-sync] $(date -Iseconds) CAS upload failed (exit=$?), will retry next cycle"
            fi

            # Sync metadata (small, fast)
            rclone copy "${BST_CACHE}/artifacts/" "r2:${R2_BUCKET}/artifacts/" \
              --no-traverse --size-only --transfers=8 --s3-no-head -q 2>&1 || true
            rclone copy "${BST_CACHE}/source_protos/" "r2:${R2_BUCKET}/source_protos/" \
              --no-traverse --size-only --transfers=8 --s3-no-head -q 2>&1 || true

            echo "[r2-sync] $(date -Iseconds) Sync cycle complete"
          done
          SYNCSCRIPT
          chmod +x /tmp/r2-sync-loop.sh

          nohup /tmp/r2-sync-loop.sh "$BST_CACHE" "$R2_BUCKET" \
            > /tmp/r2-sync-loop.log 2>&1 &
          echo $! > /tmp/r2-sync-loop.pid
          echo "Background R2 sync started (PID $(cat /tmp/r2-sync-loop.pid))"

      # ── BuildStream build ─────────────────────────────────────────────
      # Uses the Justfile's `bst` wrapper to run BuildStream inside the
      # bst2 container. CI-specific flags (--no-interactive, --config)
      # are injected via BST_FLAGS env var.
      # Additional --log-file flag is passed directly (build-specific).

      - name: Build OCI image with BuildStream
        env:
          BST_FLAGS: --no-interactive --config /src/buildstream-ci.conf
        run: |
          just bst --log-file /src/logs/build.log build oci/bluefin.bst
        timeout-minutes: 120

      - name: Disk and cache usage after build
        if: always()
        run: |
          echo "=== Disk usage ==="
          df -h /
          echo "=== BuildStream cache breakdown ==="
          du -sh ~/.cache/buildstream/{cas,artifacts,source_protos,sources,logs,build,tmp} 2>/dev/null || true
          echo "=== Total cache ==="
          du -sh ~/.cache/buildstream/ 2>/dev/null || true

      # ── Final R2 sync ───────────────────────────────────────────────
      # Stop the background sync loop, then upload the definitive cache.
      # CAS is uploaded as a single tar+zstd archive (replaces individual files).
      # Metadata dirs (artifacts, source_protos) are synced per-file (small, ~12MB).
      # Always runs (even on build failure) so partial progress is saved.

      - name: Final sync to R2
        if: always()
        continue-on-error: true
        env:
          R2_ACCESS_KEY: ${{ secrets.R2_ACCESS_KEY }}
          R2_SECRET_KEY: ${{ secrets.R2_SECRET_KEY }}
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
        run: |
          # Stop background sync loop
          if [ -f /tmp/r2-sync-loop.pid ]; then
            PID=$(cat /tmp/r2-sync-loop.pid)
            if kill -0 "$PID" 2>/dev/null; then
              echo "Stopping background R2 sync (PID $PID)"
              kill "$PID" || true
              wait "$PID" 2>/dev/null || true
            fi
            rm -f /tmp/r2-sync-loop.pid
          fi

          echo "=== Background sync log (last 30 lines) ==="
          tail -30 /tmp/r2-sync-loop.log 2>/dev/null || echo "(no log)"

          if [ -z "${R2_ACCESS_KEY}" ]; then
            echo "R2 secrets not configured, skipping final sync"
            exit 0
          fi

          BST_CACHE="$HOME/.cache/buildstream"

          echo "=== Local cache sizes ==="
          du -sh "${BST_CACHE}/cas" "${BST_CACHE}/artifacts" \
                 "${BST_CACHE}/source_protos" "${BST_CACHE}/sources" 2>/dev/null || true

          echo "=== Final CAS sync to R2 (tar+zstd stream) ==="
          # Upload entire CAS dir as a single zstd-compressed tar archive.
          # tar -> zstd (level 9, all threads) -> rclone rcat (streaming multipart upload)
          # Level 9: ~3.2x ratio vs ~2.8x at level 3, saving ~3-4 GB on R2 per archive.
          # Costs ~2-3 min extra compression time (4 cores) but decompression is the same
          # speed at any level (~1.5 GB/s), so every restore benefits from the smaller file.
          # --streaming-upload-cutoff 0: force immediate multipart (don't buffer in RAM)
          # This replaces ~20,000 individual PUT requests with a single multipart upload.
          UPLOAD_START=$SECONDS
          tar cf - -C "${BST_CACHE}/" cas/ \
            | zstd -T0 -9 \
            | rclone rcat \
                --streaming-upload-cutoff 0 \
                --s3-chunk-size=64M \
                --s3-upload-concurrency=4 \
                "r2:${R2_BUCKET}/cas.tar.zst" \
            || echo "::warning::CAS tar+zstd upload to R2 failed"
          echo "CAS upload completed in $(( SECONDS - UPLOAD_START ))s"

          echo "=== Final artifact refs sync to R2 ==="
          rclone sync "${BST_CACHE}/artifacts/" "r2:${R2_BUCKET}/artifacts/" \
            --size-only \
            --transfers=16 \
            --checkers=8 \
            --fast-list \
            --s3-no-head \
            --s3-no-system-metadata \
            -v || echo "::warning::Artifact refs sync to R2 failed"

          echo "=== Final source protos sync to R2 ==="
          rclone sync "${BST_CACHE}/source_protos/" "r2:${R2_BUCKET}/source_protos/" \
            --size-only \
            --transfers=16 \
            --checkers=8 \
            --fast-list \
            --s3-no-head \
            --s3-no-system-metadata \
            -v || echo "::warning::Source protos sync to R2 failed"

          echo "=== R2 final sync complete ==="

      # ── Export OCI image ──────────────────────────────────────────────
      # Uses the Justfile's `export` recipe: checks out the OCI image
      # layout from BuildStream, loads it into podman via skopeo, and
      # applies the /usr/etc bootc fixup. Same logic as local builds.

      - name: Export OCI image from BuildStream
        id: export
        env:
          BST_FLAGS: --no-interactive --config /src/buildstream-ci.conf
          BUILD_IMAGE_NAME: ${{ env.IMAGE_REGISTRY }}/${{ env.IMAGE_NAME }}
        run: |
          just export
          echo "image_ref=${{ env.IMAGE_REGISTRY }}/${{ env.IMAGE_NAME }}:latest" >> "$GITHUB_OUTPUT"

      - name: Verify image loaded
        run: podman images

      # ── Validation ────────────────────────────────────────────────────

      - name: Validate with bootc container lint
        run: |
          podman run --rm --privileged \
            -v /var/lib/containers:/var/lib/containers \
            "${{ steps.export.outputs.image_ref }}" \
            bootc container lint

      # ── Upload build logs ─────────────────────────────────────────────
      # Always upload, even on failure, so build failures can be diagnosed.

      - name: Upload build logs
        if: always()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
        with:
          name: buildstream-logs
          path: logs/
          retention-days: 7
          if-no-files-found: ignore

      # ── Publish to GHCR (main branch only) ───────────────────────────

      - name: Login to GHCR
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          echo "${{ secrets.GITHUB_TOKEN }}" | \
            podman login ghcr.io --username ${{ github.actor }} --password-stdin

      - name: Tag image for GHCR
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          podman tag "${{ steps.export.outputs.image_ref }}" \
            "${{ env.IMAGE_REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}"

      - name: Push to GHCR
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          podman push --retry 3 "${{ env.IMAGE_REGISTRY }}/${{ env.IMAGE_NAME }}:latest"
          podman push --retry 3 "${{ env.IMAGE_REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}"
