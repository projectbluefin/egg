name: Build Bluefin Egg

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

env:
  IMAGE_NAME: egg
  IMAGE_REGISTRY: ghcr.io/${{ github.repository_owner }}
  BST2_IMAGE: registry.gitlab.com/freedesktop-sdk/infrastructure/freedesktop-sdk-docker-images/bst2:f89b4aef847ef040b345acceda15a850219eb8f1
  R2_BUCKET: bst-cache

# On PRs: group by branch so new pushes cancel stale runs.
# On main: group by SHA so every push gets its own non-cancellable run.
# (GitHub cancels even pending/queued runs in the same group regardless of
#  cancel-in-progress when a newer run arrives, so main needs unique groups.)
concurrency:
  group: ${{ github.workflow }}-${{ github.event_name == 'pull_request' && github.ref || github.sha }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

jobs:
  build:
    runs-on: blacksmith-4vcpu-ubuntu-2404
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Pull BuildStream container image
        run: podman pull "$BST2_IMAGE"

      - name: Mount BuildStream cache (sticky disk)
        uses: useblacksmith/stickydisk@v1
        with:
          key: ${{ github.repository }}-bst-cache
          path: ~/.cache/buildstream

      - name: Prepare BuildStream cache layout
        run: |
          mkdir -p ~/.cache/buildstream/{cas,artifacts,source_protos,sources}
          echo "=== Sticky disk status ==="
          du -sh ~/.cache/buildstream/{cas,artifacts,source_protos,sources} 2>/dev/null || true
          df -h ~/.cache/buildstream

      - name: Preseed CAS from R2 (cold cache only)
        env:
          R2_ACCESS_KEY: ${{ secrets.R2_ACCESS_KEY }}
          R2_SECRET_KEY: ${{ secrets.R2_SECRET_KEY }}
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
        run: |
          BST_CACHE="$HOME/.cache/buildstream"
          CAS_OBJECTS=$(find "${BST_CACHE}/cas" -type f 2>/dev/null | head -5 | wc -l)

          if [ "$CAS_OBJECTS" -gt 0 ]; then
            echo "Sticky disk has cached CAS objects -- skipping preseed"
            echo "CAS size: $(du -sh "${BST_CACHE}/cas" | cut -f1)"
            ARTIFACT_REFS=$(find "${BST_CACHE}/artifacts" -type f 2>/dev/null | wc -l)
            echo "Artifact refs: ${ARTIFACT_REFS}"
            exit 0
          fi

          echo "Sticky disk is cold -- preseeding from R2 archive"

          if [ -z "${R2_ACCESS_KEY}" ]; then
            echo "::warning::R2 secrets not configured and sticky disk is cold -- full build expected (~2h)"
            exit 0
          fi

          # Install rclone (only needed for preseed on cold disk)
          curl -fsSL https://rclone.org/install.sh | sudo bash

          # Configure rclone for Cloudflare R2
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<RCONF
          [r2]
          type = s3
          provider = Cloudflare
          access_key_id = ${R2_ACCESS_KEY}
          secret_access_key = ${R2_SECRET_KEY}
          endpoint = ${R2_ENDPOINT}
          no_check_bucket = true
          RCONF
          sed -i 's/^[[:space:]]*//' ~/.config/rclone/rclone.conf

          # Check if R2 has a CAS archive
          CAS_REMOTE_SIZE=$(rclone size --json "r2:${R2_BUCKET}/cas.tar.zst" 2>/dev/null | jq -r '.bytes // 0')
          if [ "${CAS_REMOTE_SIZE:-0}" -le 0 ]; then
            echo "::warning::No cas.tar.zst in R2 -- cold build expected"
            exit 0
          fi

          echo "Downloading cas.tar.zst ($((CAS_REMOTE_SIZE / 1048576)) MB)..."
          TEMP_CAS=$(mktemp /tmp/cas.tar.zst.XXXXXX)
          if rclone copyto "r2:${R2_BUCKET}/cas.tar.zst" "${TEMP_CAS}" --progress; then
            ACTUAL_SIZE=$(stat --format=%s "${TEMP_CAS}" 2>/dev/null || echo 0)
            if [ "$ACTUAL_SIZE" -lt 1000 ]; then
              echo "::warning::Downloaded file is suspiciously small (${ACTUAL_SIZE} bytes) -- cold build"
            else
              echo "Validating archive integrity..."
              if zstd -t "${TEMP_CAS}"; then
                echo "Extracting into sticky disk..."
                zstd -d "${TEMP_CAS}" | tar xf - -C "${BST_CACHE}/"
                echo "CAS preseed complete: $(du -sh "${BST_CACHE}/cas" | cut -f1)"
              else
                echo "::warning::Archive validation failed -- cold build"
              fi
            fi
          else
            echo "::warning::R2 download failed -- cold build"
          fi
          rm -f "${TEMP_CAS}"

          # Also restore artifact refs and source protos metadata
          rclone copy "r2:${R2_BUCKET}/artifacts/" "${BST_CACHE}/artifacts/" \
            --size-only --transfers=16 --fast-list -q || true
          rclone copy "r2:${R2_BUCKET}/source_protos/" "${BST_CACHE}/source_protos/" \
            --size-only --transfers=16 --fast-list -q || true

          echo ""
          echo "=== Preseed summary ==="
          echo "CAS:       $(du -sh "${BST_CACHE}/cas" 2>/dev/null | cut -f1 || echo 'empty')"
          echo "Artifacts: $(find "${BST_CACHE}/artifacts" -type f 2>/dev/null | wc -l) refs"
          echo "Sources:   $(du -sh "${BST_CACHE}/sources" 2>/dev/null | cut -f1 || echo 'empty')"

      - name: Install just
        run: |
          # just is in ubuntu 24.04 universe; skopeo/podman are pre-installed
          sudo apt-get install -y just
          just --version

      # ── Generate CI-specific BuildStream config ───────────────────────
      # Tuned per gnome-build-meta CI patterns:
      # - on-error: continue  -> find ALL failures, don't stop at first
      # - fetchers: 12        -> parallel downloads from upstream caches
      # - builders: 1         -> conservative to avoid OOM
      # - retry-failed: True  -> auto-retry flaky builds
      # - error-lines: 80     -> generous error context in logs
      # - cache-buildtrees: never -> save disk (we only need final artifacts)
      #
      # No remote artifact server is configured. BuildStream uses only:
      # 1. Local disk cache (persisted by sticky disks)
      # 2. Upstream GNOME caches defined in project.conf (read-only)

      - name: Generate BuildStream CI config
        run: |
          mkdir -p logs
          cat > buildstream-ci.conf <<'BSTCONF'
          scheduler:
            on-error: continue
            fetchers: 12
            builders: 1
            network-retries: 3

          logging:
            message-format: '[%{wallclock}][%{elapsed}][%{key}][%{element}] %{action} %{message}'
            error-lines: 80

          build:
            max-jobs: 0
            retry-failed: True

          cache:
            cache-buildtrees: never
          BSTCONF

          echo "=== BuildStream CI config ==="
          cat buildstream-ci.conf

      # ── BuildStream build ─────────────────────────────────────────────
      # Uses the Justfile's `bst` wrapper to run BuildStream inside the
      # bst2 container. CI-specific flags (--no-interactive, --config)
      # are injected via BST_FLAGS env var.
      # Additional --log-file flag is passed directly (build-specific).

      - name: Build OCI image with BuildStream
        env:
          BST_FLAGS: --no-interactive --config /src/buildstream-ci.conf
        run: |
          just bst --log-file /src/logs/build.log build oci/bluefin.bst
        timeout-minutes: 120

      - name: Cache and disk status
        if: always()
        run: |
          echo "=== Disk usage ==="
          df -h ~/.cache/buildstream / 2>/dev/null || df -h /
          echo ""
          echo "=== BuildStream cache breakdown ==="
          du -sh ~/.cache/buildstream/{cas,artifacts,source_protos,sources} 2>/dev/null || true
          echo ""
          echo "=== Total ==="
          du -sh ~/.cache/buildstream/ 2>/dev/null || true

      # ── Export OCI image ──────────────────────────────────────────────
      # Uses the Justfile's `export` recipe: checks out the OCI image
      # layout from BuildStream, loads it into podman via skopeo, and
      # applies the /usr/etc bootc fixup. Same logic as local builds.
      # Creates image with local name (egg:latest) to avoid registry
      # lookups during validation.

      - name: Export OCI image from BuildStream
        id: export
        env:
          BST_FLAGS: --no-interactive --config /src/buildstream-ci.conf
          BUILD_IMAGE_NAME: ${{ env.IMAGE_NAME }}
        run: |
          just export
          echo "image_ref=${{ env.IMAGE_NAME }}:latest" >> "$GITHUB_OUTPUT"

      - name: Verify image loaded
        run: podman images

      # ── Validation ────────────────────────────────────────────────────

      - name: Validate with bootc container lint
        run: |
          podman run --rm --privileged \
            -v /var/lib/containers:/var/lib/containers \
            "localhost/${{ steps.export.outputs.image_ref }}" \
            bootc container lint

      # ── Upload build logs ─────────────────────────────────────────────
      # Always upload, even on failure, so build failures can be diagnosed.

      - name: Upload build logs
        if: always()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
        with:
          name: buildstream-logs
          path: logs/
          retention-days: 7
          if-no-files-found: ignore

      # ── Publish to GHCR (main branch only) ───────────────────────────

      - name: Login to GHCR
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          echo "${{ secrets.GITHUB_TOKEN }}" | \
            podman login ghcr.io --username ${{ github.actor }} --password-stdin

      - name: Tag image for GHCR
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          podman tag "${{ steps.export.outputs.image_ref }}" \
            "${{ env.IMAGE_REGISTRY }}/${{ env.IMAGE_NAME }}:latest"
          podman tag "${{ steps.export.outputs.image_ref }}" \
            "${{ env.IMAGE_REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}"

      - name: Push to GHCR
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          podman push --retry 3 "${{ env.IMAGE_REGISTRY }}/${{ env.IMAGE_NAME }}:latest"
          podman push --retry 3 "${{ env.IMAGE_REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}"
