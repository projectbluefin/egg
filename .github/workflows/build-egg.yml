name: Build Bluefin Egg

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

env:
  IMAGE_NAME: egg
  IMAGE_REGISTRY: ghcr.io/${{ github.repository_owner }}
  BST2_IMAGE: registry.gitlab.com/freedesktop-sdk/infrastructure/freedesktop-sdk-docker-images/bst2:f89b4aef847ef040b345acceda15a850219eb8f1
  R2_BUCKET: bst-cache

# On PRs: group by branch so new pushes cancel stale runs.
# On main: group by SHA so every push gets its own non-cancellable run.
# (GitHub cancels even pending/queued runs in the same group regardless of
#  cancel-in-progress when a newer run arrives, so main needs unique groups.)
concurrency:
  group: ${{ github.workflow }}-${{ github.event_name == 'pull_request' && github.ref || github.sha }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

jobs:
  build:
    runs-on: ubuntu-24.04
    permissions:
      contents: read
      packages: write
    steps:
      # ── Host-level setup ──────────────────────────────────────────────
      # These steps MUST run on the host, not inside the bst2 container.
      # We cannot use `container:` at job level because
      # ublue-os/remove-unwanted-software needs host filesystem access.

      - name: Free disk space
        uses: ublue-os/remove-unwanted-software@695eb75bc387dbcd9685a8e72d23439d8686cba6

      - name: Checkout repository
        uses: actions/checkout@34e114876b0b11c390a56381ad16ebd13914f8d5 # v4

      - name: Pull BuildStream container image
        run: podman pull "$BST2_IMAGE"

      - name: Cache BuildStream sources
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830 # v4
        with:
          path: ~/.cache/buildstream/sources
          key: bst-sources-${{ hashFiles('elements/**/*.bst', 'project.conf') }}
          restore-keys: |
            bst-sources-

      - name: Prepare BuildStream cache directory
        run: |
          mkdir -p "$HOME/.cache/buildstream/sources"
          mkdir -p "$HOME/.cache/buildstream/cas"
          mkdir -p "$HOME/.cache/buildstream/artifacts"
          mkdir -p "$HOME/.cache/buildstream/source_protos"

      # ── Restore cache from R2 ─────────────────────────────────────────
      # CAS objects (the bulk of the cache, ~20GB+) are stored as a single
      # zstd-compressed tar archive in R2. This is orders of magnitude
      # faster than downloading 20,000+ individual files:
      #   - 1 GET request instead of ~20,000 GETs + 256 LISTs + 20,000 HEADs
      #   - Pure streaming I/O (rclone cat | zstd -d | tar x)
      #   - zstd decompression at ~1.5 GB/s is faster than network delivery
      #   - ~21GB compresses to ~8-12GB, cutting transfer time in half
      #
      # Artifact refs and source protos are small metadata dirs (~12MB)
      # kept as individual files for incremental sync.

      - name: Install rclone
        run: |
          curl -fsSL https://rclone.org/install.sh | sudo bash
          rclone version

      - name: Restore BuildStream cache from R2
        env:
          R2_ACCESS_KEY: ${{ secrets.R2_ACCESS_KEY }}
          R2_SECRET_KEY: ${{ secrets.R2_SECRET_KEY }}
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
        run: |
          if [ -z "${R2_ACCESS_KEY}" ]; then
            echo "R2 secrets not configured, skipping cache restore"
            exit 0
          fi

          # Configure rclone for Cloudflare R2 (S3-compatible)
          mkdir -p ~/.config/rclone
          cat > ~/.config/rclone/rclone.conf <<EOF
          [r2]
          type = s3
          provider = Cloudflare
          access_key_id = $R2_ACCESS_KEY
          secret_access_key = $R2_SECRET_KEY
          endpoint = $R2_ENDPOINT
          no_check_bucket = true
          EOF
          # Strip leading whitespace from config (heredoc is indented in YAML)
          sed -i 's/^[[:space:]]*//' ~/.config/rclone/rclone.conf

          BST_CACHE="$HOME/.cache/buildstream"

          echo "=== Restoring CAS objects from R2 (tar+zstd stream) ==="
          # Single GET request for the entire CAS instead of ~20,000 individual GETs.
          # Stream: R2 -> rclone cat -> zstd decompress -> tar extract
          # Falls back gracefully on first run when cas.tar.zst doesn't exist yet.
          RESTORE_START=$SECONDS
          if rclone lsf "r2:${R2_BUCKET}/cas.tar.zst" >/dev/null 2>&1; then
            if rclone cat "r2:${R2_BUCKET}/cas.tar.zst" \
              --s3-no-head-object \
              | zstd -d \
              | tar xf - -C "${BST_CACHE}/"; then
              echo "CAS restore completed in $(( SECONDS - RESTORE_START ))s"
            else
              echo "::warning::CAS tar+zstd restore failed (non-fatal, will build from scratch)"
            fi
          else
            echo "No cas.tar.zst found in R2 (first run?), skipping CAS restore"
          fi

          echo "=== Restoring artifact refs from R2 ==="
          rclone copy "r2:${R2_BUCKET}/artifacts/" "${BST_CACHE}/artifacts/" \
            --size-only \
            --transfers=16 \
            --checkers=8 \
            --fast-list \
            --s3-no-head-object \
            --s3-no-system-metadata \
            --no-update-modtime \
            -v || echo "::warning::Artifact refs restore from R2 failed (non-fatal)"

          echo "=== Restoring source protos from R2 ==="
          rclone copy "r2:${R2_BUCKET}/source_protos/" "${BST_CACHE}/source_protos/" \
            --size-only \
            --transfers=16 \
            --checkers=8 \
            --fast-list \
            --s3-no-head-object \
            --s3-no-system-metadata \
            --no-update-modtime \
            -v || echo "::warning::Source protos restore from R2 failed (non-fatal)"

          echo "=== Cache restore summary ==="
          du -sh "${BST_CACHE}/cas" "${BST_CACHE}/artifacts" \
                 "${BST_CACHE}/source_protos" "${BST_CACHE}/sources" 2>/dev/null || true
          df -h /

      - name: Disk space before build
        run: df -h /

      # ── Generate CI-specific BuildStream config ───────────────────────
      # Tuned per gnome-build-meta CI patterns:
      # - on-error: continue  -> find ALL failures, don't stop at first
      # - fetchers: 12        -> parallel downloads from upstream caches
      # - builders: 1         -> GHA has 4 vCPUs; single builder avoids OOM
      # - retry-failed: True  -> auto-retry flaky builds
      # - error-lines: 80     -> generous error context in logs
      # - cache-buildtrees: never -> save disk (we only need final artifacts)
      #
      # No remote artifact server is configured. BuildStream uses only:
      # 1. Local disk cache (restored from R2 above)
      # 2. Upstream GNOME caches defined in project.conf (read-only)
      # After the build, rclone syncs everything back to R2.

      - name: Generate BuildStream CI config
        run: |
          mkdir -p logs
          cat > buildstream-ci.conf <<'BSTCONF'
          scheduler:
            on-error: continue
            fetchers: 12
            builders: 1
            network-retries: 3

          logging:
            message-format: '[%{wallclock}][%{elapsed}][%{key}][%{element}] %{action} %{message}'
            error-lines: 80

          build:
            max-jobs: 0
            retry-failed: True

          cache:
            cache-buildtrees: never
          BSTCONF

          echo "=== BuildStream CI config ==="
          cat buildstream-ci.conf

      # ── Background R2 sync ──────────────────────────────────────────
      # Incrementally push new CAS blobs to R2 every 2 minutes while the
      # build runs, using per-file rclone copy. This ensures partial
      # progress is saved even if the build times out or is cancelled.
      # The final sync step creates the definitive tar+zstd archive;
      # these per-file uploads serve as a safety net only.
      # The loop runs on the host (not in the container) and has direct
      # access to ~/.cache/buildstream/ via the bind mount.

      - name: Start background R2 sync
        env:
          R2_ACCESS_KEY: ${{ secrets.R2_ACCESS_KEY }}
          R2_SECRET_KEY: ${{ secrets.R2_SECRET_KEY }}
        run: |
          if [ -z "${R2_ACCESS_KEY}" ]; then
            echo "R2 secrets not configured, skipping background sync"
            exit 0
          fi

          BST_CACHE="$HOME/.cache/buildstream"

          cat > /tmp/r2-sync-loop.sh <<'SYNCSCRIPT'
          #!/usr/bin/env bash
          set -euo pipefail
          BST_CACHE="$1"
          R2_BUCKET="$2"
          INTERVAL=120  # seconds between sync cycles

          echo "[r2-sync] Background sync started (every ${INTERVAL}s)"
          while true; do
            sleep "$INTERVAL"
            echo "[r2-sync] $(date -Iseconds) Starting sync cycle..."

            # CAS: copy only (never delete), skip existing (immutable blobs)
            # --no-traverse: don't list R2, just check each local file individually
            # --size-only: filename=SHA256 hash, size match = correct
            rclone copy "${BST_CACHE}/cas/" "r2:${R2_BUCKET}/cas/" \
              --no-traverse \
              --size-only \
              --transfers=16 \
              --checkers=4 \
              --s3-no-head \
              --s3-upload-concurrency=4 \
              --s3-chunk-size=16M \
              --s3-no-system-metadata \
              --stats=0 \
              -q 2>&1 | sed 's/^/[r2-sync] /' || echo "[r2-sync] CAS sync warning (non-fatal)"

            # Artifact refs: copy new/updated refs
            rclone copy "${BST_CACHE}/artifacts/" "r2:${R2_BUCKET}/artifacts/" \
              --no-traverse \
              --size-only \
              --transfers=8 \
              --checkers=4 \
              --s3-no-head \
              --s3-no-system-metadata \
              --stats=0 \
              -q 2>&1 | sed 's/^/[r2-sync] /' || echo "[r2-sync] Artifacts sync warning (non-fatal)"

            # Source protos: copy new/updated protos
            rclone copy "${BST_CACHE}/source_protos/" "r2:${R2_BUCKET}/source_protos/" \
              --no-traverse \
              --size-only \
              --transfers=8 \
              --checkers=4 \
              --s3-no-head \
              --s3-no-system-metadata \
              --stats=0 \
              -q 2>&1 | sed 's/^/[r2-sync] /' || echo "[r2-sync] Source protos sync warning (non-fatal)"

            echo "[r2-sync] $(date -Iseconds) Sync cycle complete"
          done
          SYNCSCRIPT
          chmod +x /tmp/r2-sync-loop.sh

          nohup /tmp/r2-sync-loop.sh "$BST_CACHE" "$R2_BUCKET" \
            > /tmp/r2-sync-loop.log 2>&1 &
          echo $! > /tmp/r2-sync-loop.pid
          echo "Background R2 sync started (PID $(cat /tmp/r2-sync-loop.pid))"

      # ── BuildStream build ─────────────────────────────────────────────
      # Runs inside the bst2 container with:
      # - --privileged: required for bubblewrap sandboxing
      # - --device /dev/fuse: required for buildbox-fuse (ext4 lacks reflinks)
      # - ulimit -n 1048576: buildbox-casd needs many file descriptors
      # - --no-interactive: prevent blocking on prompts in CI

      - name: Build OCI image with BuildStream
        run: |
          podman run --rm \
            --privileged \
            --device /dev/fuse \
            --network=host \
            -v "${{ github.workspace }}:/src:rw" \
            -v "$HOME/.cache/buildstream:/root/.cache/buildstream:rw" \
            -w /src \
            "$BST2_IMAGE" \
            bash -c '
              ulimit -n 1048576 || true
              bst --no-interactive \
                  --colors \
                  --config /src/buildstream-ci.conf \
                  --log-file /src/logs/build.log \
                  build \
                  oci/bluefin.bst
            '
        timeout-minutes: 120

      # ── Final R2 sync ───────────────────────────────────────────────
      # Stop the background sync loop, then upload the definitive cache.
      # CAS is uploaded as a single tar+zstd archive (replaces individual files).
      # Metadata dirs (artifacts, source_protos) are synced per-file (small, ~12MB).
      # Always runs (even on build failure) so partial progress is saved.

      - name: Final sync to R2
        if: always()
        continue-on-error: true
        env:
          R2_ACCESS_KEY: ${{ secrets.R2_ACCESS_KEY }}
          R2_SECRET_KEY: ${{ secrets.R2_SECRET_KEY }}
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
        run: |
          # Stop background sync loop
          if [ -f /tmp/r2-sync-loop.pid ]; then
            PID=$(cat /tmp/r2-sync-loop.pid)
            if kill -0 "$PID" 2>/dev/null; then
              echo "Stopping background R2 sync (PID $PID)"
              kill "$PID" || true
              wait "$PID" 2>/dev/null || true
            fi
            rm -f /tmp/r2-sync-loop.pid
          fi

          echo "=== Background sync log (last 30 lines) ==="
          tail -30 /tmp/r2-sync-loop.log 2>/dev/null || echo "(no log)"

          if [ -z "${R2_ACCESS_KEY}" ]; then
            echo "R2 secrets not configured, skipping final sync"
            exit 0
          fi

          BST_CACHE="$HOME/.cache/buildstream"

          echo "=== Local cache sizes ==="
          du -sh "${BST_CACHE}/cas" "${BST_CACHE}/artifacts" \
                 "${BST_CACHE}/source_protos" "${BST_CACHE}/sources" 2>/dev/null || true

          echo "=== Final CAS sync to R2 (tar+zstd stream) ==="
          # Upload entire CAS dir as a single zstd-compressed tar archive.
          # tar -> zstd (level 3, all threads) -> rclone rcat (streaming multipart upload)
          # --streaming-upload-cutoff 0: force immediate multipart (don't buffer in RAM)
          # This replaces ~20,000 individual PUT requests with a single multipart upload.
          UPLOAD_START=$SECONDS
          tar cf - -C "${BST_CACHE}/" cas/ \
            | zstd -T0 -3 \
            | rclone rcat \
                --streaming-upload-cutoff 0 \
                --s3-chunk-size=64M \
                --s3-upload-concurrency=4 \
                "r2:${R2_BUCKET}/cas.tar.zst" \
            || echo "::warning::CAS tar+zstd upload to R2 failed"
          echo "CAS upload completed in $(( SECONDS - UPLOAD_START ))s"

          echo "=== Final artifact refs sync to R2 ==="
          rclone sync "${BST_CACHE}/artifacts/" "r2:${R2_BUCKET}/artifacts/" \
            --size-only \
            --transfers=16 \
            --checkers=8 \
            --fast-list \
            --s3-no-head \
            --s3-no-system-metadata \
            -v || echo "::warning::Artifact refs sync to R2 failed"

          echo "=== Final source protos sync to R2 ==="
          rclone sync "${BST_CACHE}/source_protos/" "r2:${R2_BUCKET}/source_protos/" \
            --size-only \
            --transfers=16 \
            --checkers=8 \
            --fast-list \
            --s3-no-head \
            --s3-no-system-metadata \
            -v || echo "::warning::Source protos sync to R2 failed"

          echo "=== R2 final sync complete ==="

      - name: Disk space after build
        if: always()
        run: df -h /

      # ── Export OCI image ──────────────────────────────────────────────
      # Stream the OCI tar from BuildStream directly into podman on the
      # host. This avoids writing an intermediate tar file to disk.
      # Pattern: bst artifact checkout --tar - | podman load

      - name: Export OCI image from BuildStream
        id: export
        run: |
          LOADED=$(podman run --rm \
            --privileged \
            --device /dev/fuse \
            -v "${{ github.workspace }}:/src:rw" \
            -v "$HOME/.cache/buildstream:/root/.cache/buildstream:rw" \
            -w /src \
            "$BST2_IMAGE" \
            bash -c '
              ulimit -n 1048576 || true
              bst --no-interactive \
                  --config /src/buildstream-ci.conf \
                  artifact checkout --tar - oci/bluefin.bst
            ' | podman load)
          # podman load prints "Loaded image: <name>:<tag>" or "Loaded image(s): <id>"
          IMAGE_REF=$(echo "$LOADED" | grep -oP '(?<=Loaded image: ).*' || \
                      echo "$LOADED" | grep -oP '(?<=Loaded image\(s\): ).*')
          echo "image_ref=$IMAGE_REF" >> "$GITHUB_OUTPUT"
          echo "Loaded: $IMAGE_REF"

      - name: Verify image loaded
        run: podman images

      # ── Validation ────────────────────────────────────────────────────

      - name: Validate with bootc container lint
        run: |
          podman run --rm --privileged \
            -v /var/lib/containers:/var/lib/containers \
            "${{ steps.export.outputs.image_ref }}" \
            bootc container lint

      # ── Upload build logs ─────────────────────────────────────────────
      # Always upload, even on failure, so build failures can be diagnosed.

      - name: Upload build logs
        if: always()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
        with:
          name: buildstream-logs
          path: logs/
          retention-days: 7
          if-no-files-found: ignore

      # ── Publish to GHCR (main branch only) ───────────────────────────

      - name: Login to GHCR
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          echo "${{ secrets.GITHUB_TOKEN }}" | \
            podman login ghcr.io --username ${{ github.actor }} --password-stdin

      - name: Tag image for GHCR
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          podman tag "${{ steps.export.outputs.image_ref }}" \
            "${{ env.IMAGE_REGISTRY }}/${{ env.IMAGE_NAME }}:latest" \
            "${{ env.IMAGE_REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}"

      - name: Push to GHCR
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          podman push --retry 3 "${{ env.IMAGE_REGISTRY }}/${{ env.IMAGE_NAME }}:latest"
          podman push --retry 3 "${{ env.IMAGE_REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}"
